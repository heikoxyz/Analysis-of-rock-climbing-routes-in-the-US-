{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06c948a-991b-4d42-8edc-5d7fb30ff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we want to set the stage and import all required packages as well as the data to perform the analysis.\n",
    "# The data is imported efficiently, loading all data sets in a dictonary where each key (geography)\n",
    "# holds a list of dataframes with all route information within that geography split by state.\n",
    "# Simultaneously the names of each geography gets extracted from the folder names and the names of each state\n",
    "# from the file names.\n",
    "\n",
    "# Load the required packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Collect folder paths of the data\n",
    "all_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-usa-routes-aug-2020\"\n",
    "ca_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-ca\"\n",
    "midwest_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-midwest\"\n",
    "northeast_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-northeast\"\n",
    "southeast_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-southeast\"\n",
    "westcoast_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-westcoast\"\n",
    "m1_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-mountains1\"\n",
    "m2_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-mountain2\"\n",
    "ms2_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-mountains2\"\n",
    "\n",
    "# Extract and save the data found in all_dir that holds all route information of the USA from 2020 and save it to \"all\".\n",
    "all = pd.read_json(\"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-usa-routes-aug-2020/openbeta-usa-routes-aug-2020.jsonlines\", lines= True)\n",
    "\n",
    "\n",
    "# Create a list of all folder path strings, excluding all_dir\n",
    "folder_paths = [ca_dir, midwest_dir, northeast_dir, southeast_dir, westcoast_dir, m1_dir, m2_dir, ms2_dir]\n",
    "\n",
    "# Extract the other geographies:\n",
    "\n",
    "geographies_data = {}  # Initialize an empty dictionary to store the resulting list of dataframes for each geography\n",
    "geographies_states = {} # Initialize an empty dictionary to store the state name strings in lists for each geography\n",
    "\n",
    "# Here we fill the dictinary geography with a list of dataframes for each geograpy. Each dataframe corresponds to a US state:\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "    file_list = glob.glob(folder_path + '/*.jsonlines')  # Get a list of all JSON files in the folder\n",
    "    file_list.sort() # Sort the list aphabetically, since glob does not do it\n",
    "\n",
    "    # Extract the state names from the file names in each folder.\n",
    "    file_list_trunc = [] # Initialize empty list\n",
    "    for file in file_list : # For each file in the file_list, truncate the string to only keep the state names\n",
    "        file_trunc = file.split('-routes.jsonline', 1)[0]\n",
    "        file_trunc = file_trunc.split('\\\\', 1)[-1]\n",
    "        file_list_trunc.append(file_trunc) # Append the state name to the list of statenames\n",
    "\n",
    "    folder_data_frames = []  # List to store the dataframes for each geography\n",
    "\n",
    "    for file_name in file_list:\n",
    "        data_frame = pd.read_json(file_name, lines = True)\n",
    "        folder_data_frames.append(data_frame)\n",
    "\n",
    "        folder_name = os.path.basename(folder_path) # Extract the folder name\n",
    "        # Strip the first part of the folder name, only keeping the last element of the split.\n",
    "        folder_name = folder_name.split('routes-', 1)[-1] \n",
    "        \n",
    "    # Add the list of DataFrames and state names to the dictionary with the folder name as the key.\n",
    "    # The names need to be unique for later variable extraction, so we add _data or _state to the string.\n",
    "    \n",
    "    geographies_data[folder_name + \"_data\"] = folder_data_frames\n",
    "    geographies_states[folder_name + \"_state\"] = file_list_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d99a5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ca_data', 'midwest_data', 'northeast_data', 'southeast_data', 'westcoast_data', 'mountains1_data', 'mountain2_data', 'mountains2_data'])\n",
      "dict_keys(['ca_state', 'midwest_state', 'northeast_state', 'southeast_state', 'westcoast_state', 'mountains1_state', 'mountain2_state', 'mountains2_state'])\n"
     ]
    }
   ],
   "source": [
    "# Having the data imported, we now want to wrangle the data. \n",
    "# This code chunk saves each geography in its own variable and assigns key:value pairs to the dataframes for each state\n",
    "# Afterwards each state will get its own variable with all the route information from that state.\n",
    "\n",
    "\n",
    "# look at the keys generated for the two dictionary\n",
    "print(geographies_data.keys())\n",
    "\n",
    "print(geographies_states.keys())\n",
    "\n",
    "# Extract the keys from the dictionaries as variables with the list values defining those variables.\n",
    "# Save the list of keys in a list.\n",
    "\n",
    "list_of_state_names = []\n",
    "list_of_dataframes = []\n",
    "\n",
    "for key, value in geographies_data.items(): # geographies_data.items() provides a way to access and work with the individual key-value pairs of the dictionary.\n",
    "    globals()[key] = value \n",
    "    # This line dynamically creates a new variable within the loop to add to the global namespace with the name of the current key, \n",
    "    # and assigns the corresponding value.\n",
    "    list_of_dataframes.append(globals()[key]) # fill this list with the dataframes.\n",
    "\n",
    "for key, value in geographies_states.items():\n",
    "    globals()[key] = value\n",
    "    list_of_state_names.append(globals()[key]) \n",
    "\n",
    "# After having each region as a variable with a list of dataframes or a list of their state names,\n",
    "# we want to combine them. Each state name should become a variable for its corresponding data frame.\n",
    "# States that appear as duplicates are tracked and saved seperately.\n",
    "\n",
    "# to track the count of each state name across region and to avoid overwriting data frames of states that already appeared in another geopgraphy,\n",
    "# we need to keep track of their counts. This helps control duplicates.\n",
    "\n",
    "state_counts = {} # initialize dictionary to track state counts. \n",
    "duplicates = {} # initialize dictionary to track duplicate states. \n",
    "list_of_all_states_and_data = {} # initialze dictionary without duplicates, collecting the first occurance of each state.\n",
    "\n",
    "# Here we create a nested for-loop that iterates over the lists containing the state names and dataframes.\n",
    "# Zip allows for iteration at the same time. For each item of the lists we have lists.\n",
    "# The nested for loop-therefore iterates over the items of the lists lists.\n",
    "\n",
    "for states, dataframes in zip(list_of_state_names, list_of_dataframes) :\n",
    "    for state, dataframe in zip(states, dataframes) :\n",
    "        # Add state to the global namespace if it does not exist so far and set its count to 1.\n",
    "        if state not in globals():\n",
    "            globals()[state] = dataframe\n",
    "            state_counts[state] = 1\n",
    "            list_of_all_states_and_data[state] = dataframe\n",
    "        else:\n",
    "        # If state already exists, save the duplicate in a sperate variable, increase its count by 1 and add the count value to the variable name.   \n",
    "            count = state_counts.get(state, 1) + 1\n",
    "            state_counts[state] = count\n",
    "            state_i = state + \"_\" + str(count)\n",
    "            globals()[state_i] = dataframe\n",
    "            duplicates[state] = count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16e3e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mi': 2, 'az': 2, 'id': 2, 'mt': 2, 'nm': 2, 'nv': 2, 'wy': 2}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# This code chunk compares the duplicated state dfs and joins their data frames if they are not equal without duplication.\n",
    "# If the data is equal, we ignore the second data set of that state.\n",
    "# This code chunk then adds a State column to each data frame containing the state name, \n",
    "# to later identify from which state that route is coming from. \n",
    "# We then merge all state dfs into a single df.\n",
    "\n",
    "# First we check if data frames of state duplicates are identical.\n",
    "\n",
    "print(duplicates)\n",
    "\n",
    "are_equal = mi.equals(mi_2)\n",
    "\n",
    "print(are_equal)\n",
    "\n",
    "# Looking at the namespace and size (rows) of all duplicate data frames,\n",
    "# it is fair to assume, that all data frames in the list \"duplicates\" are actually duplicates.\n",
    "# They can therefore be ignored in further analysis and we continue with the list of all states filled earlier.\n",
    "\n",
    "\n",
    "len(list_of_all_states_and_data) # How many states\n",
    "\n",
    "# Add the key (state) as a separate column to each DataFrame\n",
    "for state, df in list_of_all_states_and_data.items():\n",
    "    df['State'] = state\n",
    "\n",
    "# Merge all dfs of the individual states into a single df containing all routes\n",
    "df_all_states = pd.concat(list_of_all_states_and_data.values(), ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "073b1695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136632\n",
      "Index(['route_name', 'grade', 'safety', 'type', 'fa', 'description',\n",
      "       'location', 'protection', 'metadata'],\n",
      "      dtype='object')\n",
      "155224\n",
      "Index(['route_name', 'grade', 'safety', 'type', 'fa', 'description',\n",
      "       'location', 'protection', 'metadata', 'mp_sector_id', 'mp_route_id',\n",
      "       'State'],\n",
      "      dtype='object')\n",
      "['Wheres Waldo?', 'Unknown', 'Vanished Edens', 'Stairway to Heaven', 'Shagadelic Humper Bumper']\n",
      "['Gravel Pit', 'Random Impulse', 'The Tick Wall', 'Orange Crush', 'Wimovi Wonder Winos']\n",
      "['', 'Midget Proof Direct', 'GM Dihedral', 'Mariscos Lambada (aka Gook-a-nook?)', 'Steers and Queers', 'Sit start', 'Overgrowth', '4 Finger Bush', 'Yank-Me-Doodle', 'Whipping Post Right (V6 Variation)', 'Runaway Slave', 'Mom Com', 'Gates of Heaven', 'Lil Nipple', 'Ole Nessie', 'French Tits', '\"Project\"', 'Action Deluxe(a.k.a the egyptian)', 'Keymaster Variation', 'The Freezer aka The Fridge']\n",
      "['Nolo Contendere', 'Bars Over Stacks', 'Peace Will Guide the Planets', 'Unknown Potential', 'Comic Sans Project', 'Copperhead Corner', 'Thumbsucker', 'Crimpin on Slimpers', 'The Hair', 'Bleu Cheese Crumbles', 'The Misfortunes of Virtue', 'Le Crimp De Beauvoir', 'Sound the Alarm (sds)', 'Bat Mantle', 'The Future Is Now', 'Route 5 Upper Wall', \"West Ramp of Wizard's Hat\", 'Timber!', 'Lucky Ledges', 'Careless Smoker']\n",
      "2669\n",
      "21261\n",
      "1.9534223315182386\n",
      "13.696979848477039\n"
     ]
    }
   ],
   "source": [
    "# This code chunk compares the two data frames, the published data frame containing all data from August 2020\n",
    "# and our generated df of all routes from all states.\n",
    "\n",
    "# Make sure there are no duplicates of routes\n",
    "all.drop_duplicates(subset = 'route_name', inplace = True)\n",
    "df_all_states.drop_duplicates(subset = 'route_name', inplace = True)\n",
    "all.reset_index(drop=True, inplace=True)\n",
    "df_all_states.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Compare the length and the columns of the two dfs.\n",
    "print(len(all))\n",
    "print(all.columns)\n",
    "print(len(df_all_states))\n",
    "print(df_all_states.columns)\n",
    "\n",
    "# For our analysis of climbing grades in the US, we are less intersted in information like First ascent or metadata.\n",
    "# We can therefore drop those columns.\n",
    " \n",
    "all.drop(columns=['fa', 'metadata', 'protection', 'safety', 'description', 'location'], inplace = True)\n",
    "df_all_states.drop(columns=['fa', 'metadata', 'protection', 'safety', 'description', 'location'], inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "all_list_route_names = all['route_name'].tolist()\n",
    "df_all_states_list_route_names = df_all_states['route_name'].tolist()\n",
    "print(all_list_route_names[0:5])\n",
    "print(df_all_states_list_route_names[0:5])\n",
    "\n",
    "# Here we want to figure out which routes are different between the two sets. \n",
    "# Since looping if statements over 200K long lists is not efficient we use the set data type which has \n",
    "# better performance for large datasets because they leverage hashing and indexing.\n",
    "\n",
    "set_all_routes = set(all_list_route_names)\n",
    "set_df_all_states_routes = set(df_all_states_list_route_names)\n",
    "\n",
    "all_unique_routes = list(set_all_routes - set_df_all_states_routes)\n",
    "df_all_states_unique_routes = list(set_df_all_states_routes - set_all_routes)\n",
    "\n",
    "print(all_unique_routes[0:20])\n",
    "print(df_all_states_unique_routes[0:20])\n",
    "\n",
    "print(len(all_unique_routes))\n",
    "print(len(df_all_states_unique_routes))\n",
    "\n",
    "perc_unique_all = len(all_unique_routes)/len(all)*100\n",
    "perc_unique_df_all_states = len(df_all_states_unique_routes)/len(df_all_states)*100\n",
    "\n",
    "print(perc_unique_all)\n",
    "print(perc_unique_df_all_states)\n",
    "\n",
    "# 1.95% of routes in \"all\" are unique for \"all\" while 13.7 % of routes are unique for the \"df_all_states\".\n",
    "# The more complete data set seems to be \"df_all_states\" while it also contains state information which we want to analyze and compare.\n",
    "# \"df_all_states\" contains unique columns holding the Mountain project sector and route id. If the \"all\" df would have the column \"mp_sector_id\" too,\n",
    "# we could join some of the data via sector_ids.\n",
    "# This is not the case. \n",
    "# For our further analysis we will therefore focus on the \"df_all_states\" dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f0bfb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code chunk will further clean the data frame to prepare the final dataset before we jump into the analysis.\n",
    "\n",
    "# Here we convert a list of dictionaries (the grade column of df_all_states) into a dataframe with the keys as column headers.\n",
    "grades_df = pd.DataFrame.from_records(df_all_states['grade'])\n",
    "\n",
    "# Here we convert a list of dictionaries (the type column of df_all_states) into a dataframe with the keys as column headers.\n",
    "type_df = pd.DataFrame.from_records(df_all_states['type'])\n",
    "\n",
    "# Add a suffix to the column names and drop grading systems we are not interested in.\n",
    "modified_columns = [col + '_grading' for col in grades_df.columns]\n",
    "grades_df.columns = modified_columns\n",
    "grades_df.drop(columns=['Font_grading', 'yds_aid_grading'], inplace = True)\n",
    "\n",
    "# merge the grades_df and type_df with df_all_states.\n",
    "frames = [df_all_states, grades_df, type_df]\n",
    "data_final = pd.concat(frames, axis = 1)\n",
    "data_final.drop(columns=['grade', 'type'], inplace = True)\n",
    "\n",
    "# Make the wording of the column header more understandable.\n",
    "data_final.rename(columns={'tr': 'toprope'}, inplace=True)\n",
    "\n",
    "# Export the final dataset to Excel for further analsis elsewhere.\n",
    "data_final.to_excel('G:\\My Drive\\Trainings\\Python\\Trainings_file\\climbing project\\Cleaned dataset\\Data_final.xlsx', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8bb5745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_name</th>\n",
       "      <th>mp_sector_id</th>\n",
       "      <th>mp_route_id</th>\n",
       "      <th>State</th>\n",
       "      <th>YDS_grading</th>\n",
       "      <th>French_grading</th>\n",
       "      <th>Ewbanks_grading</th>\n",
       "      <th>UIAA_grading</th>\n",
       "      <th>ZA_grading</th>\n",
       "      <th>British_grading</th>\n",
       "      <th>trad</th>\n",
       "      <th>boulder</th>\n",
       "      <th>sport</th>\n",
       "      <th>toprope</th>\n",
       "      <th>alpine</th>\n",
       "      <th>aid</th>\n",
       "      <th>snow</th>\n",
       "      <th>mixed</th>\n",
       "      <th>ice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gravel Pit</td>\n",
       "      <td>119029240.0</td>\n",
       "      <td>119029258.0</td>\n",
       "      <td>ca</td>\n",
       "      <td>5.12b/c</td>\n",
       "      <td>7b+</td>\n",
       "      <td>27</td>\n",
       "      <td>IX-</td>\n",
       "      <td>27</td>\n",
       "      <td>E6 6b</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Impulse</td>\n",
       "      <td>119100232.0</td>\n",
       "      <td>119101118.0</td>\n",
       "      <td>ca</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5a</td>\n",
       "      <td>15</td>\n",
       "      <td>V+</td>\n",
       "      <td>13</td>\n",
       "      <td>MVS 4b</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Tick Wall</td>\n",
       "      <td>119181845.0</td>\n",
       "      <td>119181945.0</td>\n",
       "      <td>ca</td>\n",
       "      <td>V-easy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Orange Crush</td>\n",
       "      <td>105817198.0</td>\n",
       "      <td>105817201.0</td>\n",
       "      <td>ca</td>\n",
       "      <td>5.11b/c</td>\n",
       "      <td>6c+</td>\n",
       "      <td>23</td>\n",
       "      <td>VIII-</td>\n",
       "      <td>24</td>\n",
       "      <td>E4 6a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wimovi Wonder Winos</td>\n",
       "      <td>113627837.0</td>\n",
       "      <td>118979787.0</td>\n",
       "      <td>ca</td>\n",
       "      <td>5.10-</td>\n",
       "      <td>6a</td>\n",
       "      <td>18</td>\n",
       "      <td>VI+</td>\n",
       "      <td>18</td>\n",
       "      <td>E1 5a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155219</th>\n",
       "      <td>Mr. Ziggy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wy</td>\n",
       "      <td>5.9+</td>\n",
       "      <td>5c</td>\n",
       "      <td>17</td>\n",
       "      <td>VI</td>\n",
       "      <td>17</td>\n",
       "      <td>E1 5a</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155220</th>\n",
       "      <td>Sky Route</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wy</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5b</td>\n",
       "      <td>16</td>\n",
       "      <td>VI-</td>\n",
       "      <td>15</td>\n",
       "      <td>HVS 4c</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155221</th>\n",
       "      <td>Running Out of Blow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wy</td>\n",
       "      <td>5.6</td>\n",
       "      <td>4c</td>\n",
       "      <td>14</td>\n",
       "      <td>V</td>\n",
       "      <td>12</td>\n",
       "      <td>S 4b</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155222</th>\n",
       "      <td>Red Nations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wy</td>\n",
       "      <td>5.11c</td>\n",
       "      <td>6c+</td>\n",
       "      <td>24</td>\n",
       "      <td>VIII-</td>\n",
       "      <td>24</td>\n",
       "      <td>E4 6a</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155223</th>\n",
       "      <td>Crevasse Route</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wy</td>\n",
       "      <td>5.7+</td>\n",
       "      <td>5a</td>\n",
       "      <td>15</td>\n",
       "      <td>V+</td>\n",
       "      <td>13</td>\n",
       "      <td>MVS 4b</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155224 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 route_name  mp_sector_id  mp_route_id State YDS_grading  \\\n",
       "0                Gravel Pit   119029240.0  119029258.0    ca     5.12b/c   \n",
       "1            Random Impulse   119100232.0  119101118.0    ca         5.7   \n",
       "2             The Tick Wall   119181845.0  119181945.0    ca      V-easy   \n",
       "3              Orange Crush   105817198.0  105817201.0    ca     5.11b/c   \n",
       "4       Wimovi Wonder Winos   113627837.0  118979787.0    ca       5.10-   \n",
       "...                     ...           ...          ...   ...         ...   \n",
       "155219            Mr. Ziggy           NaN          NaN    wy        5.9+   \n",
       "155220            Sky Route           NaN          NaN    wy         5.8   \n",
       "155221  Running Out of Blow           NaN          NaN    wy         5.6   \n",
       "155222          Red Nations           NaN          NaN    wy       5.11c   \n",
       "155223       Crevasse Route           NaN          NaN    wy        5.7+   \n",
       "\n",
       "       French_grading Ewbanks_grading UIAA_grading ZA_grading British_grading  \\\n",
       "0                 7b+              27          IX-         27           E6 6b   \n",
       "1                  5a              15           V+         13          MVS 4b   \n",
       "2                 NaN             NaN          NaN        NaN             NaN   \n",
       "3                 6c+              23        VIII-         24           E4 6a   \n",
       "4                  6a              18          VI+         18           E1 5a   \n",
       "...               ...             ...          ...        ...             ...   \n",
       "155219             5c              17           VI         17           E1 5a   \n",
       "155220             5b              16          VI-         15          HVS 4c   \n",
       "155221             4c              14            V         12            S 4b   \n",
       "155222            6c+              24        VIII-         24           E4 6a   \n",
       "155223             5a              15           V+         13          MVS 4b   \n",
       "\n",
       "        trad boulder sport toprope alpine  aid snow mixed  ice  \n",
       "0       True     NaN   NaN     NaN    NaN  NaN  NaN   NaN  NaN  \n",
       "1       True     NaN   NaN     NaN    NaN  NaN  NaN   NaN  NaN  \n",
       "2        NaN    True   NaN     NaN    NaN  NaN  NaN   NaN  NaN  \n",
       "3        NaN     NaN  True     NaN    NaN  NaN  NaN   NaN  NaN  \n",
       "4        NaN     NaN  True     NaN    NaN  NaN  NaN   NaN  NaN  \n",
       "...      ...     ...   ...     ...    ...  ...  ...   ...  ...  \n",
       "155219  True     NaN   NaN     NaN    NaN  NaN  NaN   NaN  NaN  \n",
       "155220  True     NaN   NaN     NaN    NaN  NaN  NaN   NaN  NaN  \n",
       "155221  True     NaN   NaN     NaN    NaN  NaN  NaN   NaN  NaN  \n",
       "155222  True     NaN   NaN     NaN    NaN  NaN  NaN   NaN  NaN  \n",
       "155223  True     NaN   NaN     NaN    NaN  NaN  NaN   NaN  NaN  \n",
       "\n",
       "[155224 rows x 19 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With a clean dataset of all climbing routes and all sport routes across states, we can now analyze:\n",
    "# The distribution of sport climbing routes across states by grade.\n",
    "    # Which state has the hardes sport climbing routes (most routes >=9a)?\n",
    "    # How many hard grades (>= 9a) are there in the states?\n",
    "    # Which state has the most sport climbing routes?\n",
    "    # Since trad climbing is very common in the US, which state has the most trad routes?\n",
    "    # Which state has the most sport climbing routes relative to all types of climbing?\n",
    "\n",
    "    # For a more fair/relevant comparison, lets consider state area:\n",
    "        # Which state has the most dense distribution of sport routes?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
