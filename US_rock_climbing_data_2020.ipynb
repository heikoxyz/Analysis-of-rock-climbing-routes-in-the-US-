{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06c948a-991b-4d42-8edc-5d7fb30ff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we want to set the stage and import all required packages as well as the data to perform the analysis.\n",
    "# The data is imported efficiently, loading all data sets in a dictonary where each key (geography)\n",
    "# holds a list of dataframes with all route information within that geography split by state.\n",
    "# Simultaneously the names of each geography gets extracted from the folder names and the names of each state\n",
    "# from the file names.\n",
    "\n",
    "# Load the required packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Collect folder paths of the data\n",
    "all_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-usa-routes-aug-2020\"\n",
    "ca_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-ca\"\n",
    "midwest_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-midwest\"\n",
    "northeast_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-northeast\"\n",
    "southeast_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-southeast\"\n",
    "westcoast_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-westcoast\"\n",
    "m1_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-mountains1\"\n",
    "m2_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-mountain2\"\n",
    "ms2_dir = \"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-routes-mountains2\"\n",
    "\n",
    "# Extract and save the data found in all_dir that holds all route information of the USA from 2020 and save it to \"all\".\n",
    "all = pd.read_json(\"G:/My Drive/Trainings/Python/Trainings_file/climbing project/openbeta-usa-routes-aug-2020/openbeta-usa-routes-aug-2020.jsonlines\", lines= True)\n",
    "\n",
    "\n",
    "# Create a list of all folder path strings, excluding all_dir\n",
    "folder_paths = [ca_dir, midwest_dir, northeast_dir, southeast_dir, westcoast_dir, m1_dir, m2_dir, ms2_dir]\n",
    "\n",
    "# Extract the other geographies:\n",
    "\n",
    "geographies_data = {}  # Initialize an empty dictionary to store the resulting list of dataframes for each geography\n",
    "geographies_states = {} # Initialize an empty dictionary to store the state name strings in lists for each geography\n",
    "\n",
    "# Here we fill the dictinary geography with a list of dataframes for each geograpy. Each dataframe corresponds to a US state:\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "    file_list = glob.glob(folder_path + '/*.jsonlines')  # Get a list of all JSON files in the folder\n",
    "    file_list.sort() # Sort the list aphabetically, since glob does not do it\n",
    "\n",
    "    # Extract the state names from the file names in each folder.\n",
    "    file_list_trunc = [] # Initialize empty list\n",
    "    for file in file_list : # For each file in the file_list, truncate the string to only keep the state names\n",
    "        file_trunc = file.split('-routes.jsonline', 1)[0]\n",
    "        file_trunc = file_trunc.split('\\\\', 1)[-1]\n",
    "        file_list_trunc.append(file_trunc) # Append the state name to the list of statenames\n",
    "\n",
    "    folder_data_frames = []  # List to store the dataframes for each geography\n",
    "\n",
    "    for file_name in file_list:\n",
    "        data_frame = pd.read_json(file_name, lines = True)\n",
    "        folder_data_frames.append(data_frame)\n",
    "\n",
    "        folder_name = os.path.basename(folder_path) # Extract the folder name\n",
    "        # Strip the first part of the folder name, only keeping the last element of the split.\n",
    "        folder_name = folder_name.split('routes-', 1)[-1] \n",
    "        \n",
    "    # Add the list of DataFrames and state names to the dictionary with the folder name as the key.\n",
    "    # The names need to be unique for later variable extraction, so we add _data or _state to the string.\n",
    "    \n",
    "    geographies_data[folder_name + \"_data\"] = folder_data_frames\n",
    "    geographies_states[folder_name + \"_state\"] = file_list_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d99a5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ca_data', 'midwest_data', 'northeast_data', 'southeast_data', 'westcoast_data', 'mountains1_data', 'mountain2_data', 'mountains2_data'])\n",
      "dict_keys(['ca_state', 'midwest_state', 'northeast_state', 'southeast_state', 'westcoast_state', 'mountains1_state', 'mountain2_state', 'mountains2_state'])\n"
     ]
    }
   ],
   "source": [
    "# Having the data imported, we now want to wrangle the data. \n",
    "# This code chunk saves each geography in its own variable and assigns key:value pairs to the dataframes for each state\n",
    "# Afterwards each state will get its own variable with all the route information from that state.\n",
    "\n",
    "\n",
    "# look at the keys generated for the two dictionary\n",
    "print(geographies_data.keys())\n",
    "\n",
    "print(geographies_states.keys())\n",
    "\n",
    "# Extract the keys from the dictionaries as variables with the list values defining those variables.\n",
    "# Save the list of keys in a list.\n",
    "\n",
    "list_of_state_names = []\n",
    "list_of_dataframes = []\n",
    "\n",
    "for key, value in geographies_data.items(): # geographies_data.items() provides a way to access and work with the individual key-value pairs of the dictionary.\n",
    "    globals()[key] = value \n",
    "    # This line dynamically creates a new variable within the loop to add to the global namespace with the name of the current key, \n",
    "    # and assigns the corresponding value.\n",
    "    list_of_dataframes.append(globals()[key]) # fill this list with the dataframes.\n",
    "\n",
    "for key, value in geographies_states.items():\n",
    "    globals()[key] = value\n",
    "    list_of_state_names.append(globals()[key]) \n",
    "\n",
    "# After having each region as a variable with a list of dataframes or a list of their state names,\n",
    "# we want to combine them. Each state name should become a variable for its corresponding data frame.\n",
    "# States that appear as duplicates are tracked and saved seperately.\n",
    "\n",
    "# to track the count of each state name across region and to avoid overwriting data frames of states that already appeared in another geopgraphy,\n",
    "# we need to keep track of their counts. This helps control duplicates.\n",
    "\n",
    "state_counts = {} # initialize dictionary to track state counts. \n",
    "duplicates = {} # initialize dictionary to track duplicate states. \n",
    "list_of_all_states_and_data = {} # initialze dictionary without duplicates, collecting the first occurance of each state.\n",
    "\n",
    "# Here we create a nested for-loop that iterates over the lists containing the state names and dataframes.\n",
    "# Zip allows for iteration at the same time. For each item of the lists we have lists.\n",
    "# The nested for loop therefore iterates over the items of the lists lists.\n",
    "\n",
    "for states, dataframes in zip(list_of_state_names, list_of_dataframes) :\n",
    "    for state, dataframe in zip(states, dataframes) :\n",
    "        # Add state to the global namespace if it does not exist so far and set its count to 1.\n",
    "        if state not in globals():\n",
    "            globals()[state] = dataframe\n",
    "            state_counts[state] = 1\n",
    "            list_of_all_states_and_data[state] = dataframe\n",
    "        else:\n",
    "        # If state already exists, save the duplicate in a sperate variable, increase its count by 1 and add the count value to the variable name.   \n",
    "            count = state_counts.get(state, 1) + 1\n",
    "            state_counts[state] = count\n",
    "            state_i = state + \"_\" + str(count)\n",
    "            globals()[state_i] = dataframe\n",
    "            duplicates[state] = count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16e3e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mi': 2, 'az': 2, 'id': 2, 'mt': 2, 'nm': 2, 'nv': 2, 'wy': 2}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# This code chunk compares the duplicated state dfs and joins their data frames if they are not equal without duplication.\n",
    "# If the data is equal, we ignore the second data set of that state.\n",
    "# This code chunk then adds a State column to each data frame containing the state name, \n",
    "# to later identify from which state that route is coming from. \n",
    "# We then merge all state dfs into a single df.\n",
    "\n",
    "# First we check if data frames of state duplicates are identical.\n",
    "\n",
    "print(duplicates)\n",
    "\n",
    "are_equal = mi.equals(mi_2)\n",
    "\n",
    "print(are_equal)\n",
    "\n",
    "# Looking at the namespace and size (rows) of all duplicate data frames,\n",
    "# it is fair to assume, that all data frames in the list \"duplicates\" are actually duplicates.\n",
    "# They can therefore be ignored in further analysis and we continue with the list of all states filled earlier.\n",
    "\n",
    "\n",
    "len(list_of_all_states_and_data) # How many states\n",
    "\n",
    "# Add the key (state) as a separate column to each DataFrame\n",
    "for state, df in list_of_all_states_and_data.items():\n",
    "    df['State'] = state\n",
    "\n",
    "# Merge all dfs of the individual states into a single df containing all routes\n",
    "df_all_states = pd.concat(list_of_all_states_and_data.values(), ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7608d0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 route_name  \\\n",
      "0                Gravel Pit   \n",
      "1            Random Impulse   \n",
      "2             The Tick Wall   \n",
      "3              Orange Crush   \n",
      "4       Wimovi Wonder Winos   \n",
      "...                     ...   \n",
      "209803  Running Out of Blow   \n",
      "209804          Red Nations   \n",
      "209805         Planet Earth   \n",
      "209806       Heart of Stone   \n",
      "209807       Crevasse Route   \n",
      "\n",
      "                                                    grade safety  \\\n",
      "0       {'YDS': '5.12b/c', 'French': '7b+', 'Ewbanks':...          \n",
      "1       {'YDS': '5.7', 'French': '5a', 'Ewbanks': '15'...          \n",
      "2                          {'YDS': 'V-easy', 'Font': '3'}          \n",
      "3       {'YDS': '5.11b/c', 'French': '6c+', 'Ewbanks':...          \n",
      "4       {'YDS': '5.10-', 'French': '6a', 'Ewbanks': '1...          \n",
      "...                                                   ...    ...   \n",
      "209803  {'YDS': '5.6', 'French': '4c', 'Ewbanks': '14'...          \n",
      "209804  {'YDS': '5.11c', 'French': '6c+', 'Ewbanks': '...          \n",
      "209805  {'YDS': '5.11a', 'French': '6c', 'Ewbanks': '2...          \n",
      "209806  {'YDS': '5.10b', 'French': '6a+', 'Ewbanks': '...          \n",
      "209807  {'YDS': '5.7+', 'French': '5a', 'Ewbanks': '15...          \n",
      "\n",
      "                     type                                                 fa  \\\n",
      "0          {'trad': True}             Jason Milford/ Matt Schutz Spring 2020   \n",
      "1          {'trad': True}                                       \"Unknown\" or   \n",
      "2       {'boulder': True}                                       7, July 2020   \n",
      "3         {'sport': True}    Wade Griffith, Sterling Killion, Scott Williams   \n",
      "4         {'sport': True}                                    Kroll and McHam   \n",
      "...                   ...                                                ...   \n",
      "209803     {'trad': True}                               Nadiak / Herder 2016   \n",
      "209804     {'trad': True}                      Greg Collins, Sue Miller 1999   \n",
      "209805     {'trad': True}                           Greg Collins et al, 1992   \n",
      "209806     {'trad': True}  Greg Collins, Hunter Dhalberg, Kevin Emery, Mi...   \n",
      "209807     {'trad': True}                            Greg Collins Sue Miller   \n",
      "\n",
      "                                              description  \\\n",
      "0       [Goes up slab on bolts to steep corner on gear...   \n",
      "1       [Some fun moves broken up by a few scree fille...   \n",
      "2       [Bouldering. Approximately 14’ tall and 20’ or...   \n",
      "3       [Pretty cool orange arete that sports some int...   \n",
      "4       [Climb the most open looking slab on the more ...   \n",
      "...                                                   ...   \n",
      "209803  [A splitter handcrack as seen from the Cowboy ...   \n",
      "209804  [This is a stellar route, perhaps one of the t...   \n",
      "209805  [This is another stellar climb by the amazing ...   \n",
      "209806  [Left of Rat Route and right of Sky Route is t...   \n",
      "209807  [Right side of Lankin Dome's South Face is the...   \n",
      "\n",
      "                                                 location  \\\n",
      "0                                                           \n",
      "1       [25 feet to the right of Deep Springs Education.]   \n",
      "2       [Park at Sycamore Creek bridge and walk upstre...   \n",
      "3       [The route is located on the far southern shou...   \n",
      "4        [Upper right of the wall, more facing the road.]   \n",
      "...                                                   ...   \n",
      "209803  [Look downward and right of the Cowboy Routes ...   \n",
      "209804  [This is on the beautiful face right of the Tr...   \n",
      "209805  [This route is identified by the prominent lef...   \n",
      "209806  [Left of Rat Route and right of Star Jumper/Sk...   \n",
      "209807  [Right of Venus and Mars, left of Right Dike R...   \n",
      "\n",
      "                                               protection  \\\n",
      "0       [Chains on top, can lower off easy. Pro to 3\" ...   \n",
      "1       [A small assortment of cams and maybe a nut or...   \n",
      "2                             [None. Bring your own pad.]   \n",
      "3                                                [7 QD's]   \n",
      "4                                                 [Bolts]   \n",
      "...                                                   ...   \n",
      "209803  [Singles in tight to big hands #1 - #3. No anc...   \n",
      "209804  [Rack of stoppers, set of Camelots to #3.  Sav...   \n",
      "209805  [Rack of stoppers, set of Camelots to #3, RPs ...   \n",
      "209806                                 [1 set plus sling]   \n",
      "209807  [1 set and quick draws. A hand sized cam prote...   \n",
      "\n",
      "                                                 metadata  mp_sector_id  \\\n",
      "0       {'left_right_seq': '999999', 'parent_lnglat': ...   119029240.0   \n",
      "1       {'left_right_seq': '1', 'parent_lnglat': [-118...   119100232.0   \n",
      "2       {'left_right_seq': '999999', 'parent_lnglat': ...   119181845.0   \n",
      "3       {'left_right_seq': '0', 'parent_lnglat': [-119...   105817198.0   \n",
      "4       {'left_right_seq': '12', 'parent_lnglat': [-11...   113627837.0   \n",
      "...                                                   ...           ...   \n",
      "209803  {'left_right_seq': '999999', 'parent_lnglat': ...           NaN   \n",
      "209804  {'left_right_seq': '5', 'parent_lnglat': [-107...           NaN   \n",
      "209805  {'left_right_seq': '6', 'parent_lnglat': [-107...           NaN   \n",
      "209806  {'left_right_seq': '3', 'parent_lnglat': [-107...           NaN   \n",
      "209807  {'left_right_seq': '8', 'parent_lnglat': [-107...           NaN   \n",
      "\n",
      "        mp_route_id State  \n",
      "0       119029258.0    ca  \n",
      "1       119101118.0    ca  \n",
      "2       119181945.0    ca  \n",
      "3       105817201.0    ca  \n",
      "4       118979787.0    ca  \n",
      "...             ...   ...  \n",
      "209803          NaN    wy  \n",
      "209804          NaN    wy  \n",
      "209805          NaN    wy  \n",
      "209806          NaN    wy  \n",
      "209807          NaN    wy  \n",
      "\n",
      "[209808 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "073b1695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136632\n",
      "Index(['route_name', 'grade', 'type'], dtype='object')\n",
      "155224\n",
      "Index(['route_name', 'grade', 'type', 'mp_sector_id', 'mp_route_id', 'State'], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['fa', 'metadata', 'protection', 'safety', 'description', 'location'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(df_all_states\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m     16\u001b[0m \u001b[39m# For our analysis of climbing grades in the US, we are less intersted in information like First ascent or metadata.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# We can therefore drop those columns.\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mall\u001b[39;49m\u001b[39m.\u001b[39;49mdrop(columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mfa\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mprotection\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msafety\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdescription\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlocation\u001b[39;49m\u001b[39m'\u001b[39;49m], inplace \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     20\u001b[0m df_all_states\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mfa\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprotection\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msafety\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m'\u001b[39m], inplace \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m all_list_route_names \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mroute_name\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\kuhnh3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   5111\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   5112\u001b[0m     labels: IndexLabel \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   5120\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5121\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5122\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5123\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5256\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5257\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   5259\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   5260\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5261\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   5262\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   5263\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   5264\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   5265\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   5266\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kuhnh3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4548\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4549\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4551\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4552\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\kuhnh3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4590\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4591\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4592\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4594\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4595\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kuhnh3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6696\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6694\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6695\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6696\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6697\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6698\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['fa', 'metadata', 'protection', 'safety', 'description', 'location'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# This code chunk compares the two data frames, the published data frame containing all data from August 2020\n",
    "# and our generated df of all routes from all states.\n",
    "\n",
    "# Make sure there are no duplicates of routes\n",
    "all.drop_duplicates(subset = 'route_name', inplace = True)\n",
    "df_all_states.drop_duplicates(subset = 'route_name', inplace = True)\n",
    "all.reset_index(drop=True, inplace=True)\n",
    "df_all_states.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Compare the length and the columns of the two dfs.\n",
    "print(len(all))\n",
    "print(all.columns)\n",
    "print(len(df_all_states))\n",
    "print(df_all_states.columns)\n",
    "\n",
    "# For our analysis of climbing grades in the US, we are less intersted in information like First ascent or metadata.\n",
    "# We can therefore drop those columns.\n",
    " \n",
    "all.drop(columns=['fa', 'metadata', 'protection', 'safety', 'description', 'location'], inplace = True)\n",
    "df_all_states.drop(columns=['fa', 'metadata', 'protection', 'safety', 'description', 'location'], inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "all_list_route_names = all['route_name'].tolist()\n",
    "df_all_states_list_route_names = df_all_states['route_name'].tolist()\n",
    "print(all_list_route_names[0:5])\n",
    "print(df_all_states_list_route_names[0:5])\n",
    "\n",
    "# Here we want to figure out which routes are different between the two sets. \n",
    "# Since looping if statements over 200K long lists is not efficient we use the set data type which has \n",
    "# better performance for large datasets because they leverage hashing and indexing.\n",
    "\n",
    "set_all_routes = set(all_list_route_names)\n",
    "set_df_all_states_routes = set(df_all_states_list_route_names)\n",
    "\n",
    "all_unique_routes = list(set_all_routes - set_df_all_states_routes)\n",
    "df_all_states_unique_routes = list(set_df_all_states_routes - set_all_routes)\n",
    "\n",
    "print(all_unique_routes[0:20])\n",
    "print(df_all_states_unique_routes[0:20])\n",
    "\n",
    "print(len(all_unique_routes))\n",
    "print(len(df_all_states_unique_routes))\n",
    "\n",
    "perc_unique_all = len(all_unique_routes)/len(all)*100\n",
    "perc_unique_df_all_states = len(df_all_states_unique_routes)/len(df_all_states)*100\n",
    "\n",
    "print(perc_unique_all)\n",
    "print(perc_unique_df_all_states)\n",
    "\n",
    "# 1.95% of routes in \"all\" are unique for \"all\" while 13.7 % of routes are unique for the \"df_all_states\".\n",
    "# The more complete data set seems to be \"df_all_states\" while it also contains state information which we want to analyze and compare.\n",
    "# \"df_all_states\" contains unique columns holding the Mountain project sector and route id. If the \"all\" df would have the column \"mp_sector_id\" too,\n",
    "# we could join some of the data via sector_ids.\n",
    "# This is not the case. \n",
    "# For our further analysis we will therefore focus on the \"df_all_states\" dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f0bfb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        index                     route_name   type  mp_sector_id  \\\n",
      "0           3                   Orange Crush  sport   105817198.0   \n",
      "1           4            Wimovi Wonder Winos  sport   113627837.0   \n",
      "2         236              Watch Tower Arete  sport   118261975.0   \n",
      "3         237           Violent Pacification  sport   117330322.0   \n",
      "4         239                        Y'ain't  sport   110143719.0   \n",
      "...       ...                            ...    ...           ...   \n",
      "40471  155212            Bitch Muscle direct  sport           NaN   \n",
      "40472  155213                   Bitch Muscle  sport           NaN   \n",
      "40473  155214                    Belly Crawl  sport           NaN   \n",
      "40474  155215          A Place to Meet Woody  sport           NaN   \n",
      "40475  155216  A Chance 4 the Tennessee Stud  sport           NaN   \n",
      "\n",
      "       mp_route_id State YDS_grading French_grading Ewbanks_grading  \\\n",
      "0      105817201.0    ca     5.11b/c            6c+              23   \n",
      "1      118979787.0    ca       5.10-             6a              18   \n",
      "2      119497541.0    ca       5.10b            6a+              19   \n",
      "3      118249858.0    ca       5.10d            6b+              21   \n",
      "4      110143727.0    ca       5.10a             6a              18   \n",
      "...            ...   ...         ...            ...             ...   \n",
      "40471          NaN    wy       5.11b             6c              23   \n",
      "40472          NaN    wy       5.10c             6b              20   \n",
      "40473          NaN    wy       5.10b            6a+              19   \n",
      "40474          NaN    wy       5.10b            6a+              19   \n",
      "40475          NaN    wy       5.12c            7b+              27   \n",
      "\n",
      "      UIAA_grading ZA_grading British_grading  \n",
      "0            VIII-         24           E4 6a  \n",
      "1              VI+         18           E1 5a  \n",
      "2             VII-         19           E2 5b  \n",
      "3             VII+         21           E3 5b  \n",
      "4              VI+         18           E1 5a  \n",
      "...            ...        ...             ...  \n",
      "40471        VIII-         23           E3 5c  \n",
      "40472          VII         20           E2 5b  \n",
      "40473         VII-         19           E2 5b  \n",
      "40474         VII-         19           E2 5b  \n",
      "40475          IX-         27           E6 6b  \n",
      "\n",
      "[40476 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# This code chunk will further clean the data frame to prepare the final dataset before we jump into the analysis.\n",
    "\n",
    "# Here we convert a list of dictionaries into a detaframe with the keys as column headers.\n",
    "grades_df = pd.DataFrame.from_records(df_all_states['grade'])\n",
    "\n",
    "# Add a suffix to the column names and drop grading systems we are not interested in.\n",
    "modified_columns = [col + '_grading' for col in grades_df.columns]\n",
    "grades_df.columns = modified_columns\n",
    "grades_df.drop(columns=['Font_grading', 'yds_aid_grading'], inplace = True)\n",
    "\n",
    "# merge the grades_df with df_all_states.\n",
    "data_final = pd.concat([df_all_states, grades_df], axis = 1)\n",
    "data_final.drop(columns=['grade'], inplace = True)\n",
    "\n",
    "# Make the 'type' data column easier to use\n",
    "data_final['type'] = data_final['type'].astype(str)\n",
    "data_final['type'] = data_final['type'].str.replace(\"{'\", \"\")\n",
    "data_final['type'] = data_final['type'].str.replace(\"': True}\", \"\")\n",
    "\n",
    "\n",
    "# Filter for only sport routes\n",
    "sport_data = data_final[data_final['type'] == 'sport']\n",
    "sport_data = sport_data.reset_index()\n",
    "print(sport_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
